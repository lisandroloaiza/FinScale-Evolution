# C4 Model - Nivel 2: Diagrama de Contenedores

> **Objetivo**: Desglosar FinScale en aplicaciones, microservicios, bases de datos y sistemas de mensajer√≠a.  
> **Audiencia**: Arquitectos de software, tech leads, equipos de desarrollo

---

## üéØ Arquitectura de Contenedores

### Descripci√≥n

FinScale GlobalLedger se descompone en:
- **Frontend**: Mobile App (React Native), Web Portal (React + WebSockets)
- **API Gateway**: Kong + OAuth2, Legacy Facade (Anti-Corruption Layer)
- **Core Services**: Payment, Ledger, Fraud (Spring WebFlux + R2DBC/TensorFlow)
- **Supporting Services**: Customer, FX, Clearing, Notification, Regulatory Reporting, Reconciliation
- **Integration Layer**: Saga Orchestrator (Temporal.io), CDC Adapter (Debezium), HSM Proxy (Netty)
- **Messaging**: Apache Kafka (event streaming, event store)
- **Databases**: PostgreSQL, Redis Cluster, Cassandra, TimescaleDB, AWS S3
- **Observability**: Jaeger (tracing), Prometheus/Grafana (metrics), ELK Stack (logs)
- **Legacy**: Monolito Java 8 + Oracle (en migraci√≥n Strangler Fig)

---

## üìê Diagrama de Contenedores (C4 Level 2)

```mermaid
flowchart TB
    %% ============================================
    %% ESTILOS
    %% ============================================
    classDef actor fill:#08427b,stroke:#052e56,color:#fff
    classDef frontend fill:#1168bd,stroke:#0b4884,color:#fff
    classDef gateway fill:#438dd5,stroke:#2e5f8f,color:#fff
    classDef core fill:#85bbf0,stroke:#5a8fc4,color:#000
    classDef support fill:#b3d9ff,stroke:#7aa3cc,color:#000
    classDef integration fill:#6c8ebf,stroke:#4a6280,color:#fff
    classDef data fill:#dae8fc,stroke:#9bb3d4,color:#000
    classDef external fill:#f8cecc,stroke:#b85450,color:#000
    classDef observability fill:#d5e8d4,stroke:#82b366,color:#000

    %% ============================================
    %% CAPA 1: ACTORES
    %% ============================================
    subgraph ACTORS[" "]
        Customer([üë§ Cliente<br/>Usuario Final]):::actor
        Operator([üë§ Operador<br/>Back Office]):::actor
        Regulator([üë§ Regulador<br/>Entidad Fiscal]):::actor
    end

    %% ============================================
    %% CAPA 2: CANALES DIGITALES
    %% ============================================
    subgraph CHANNELS["üì± CANALES DIGITALES"]
        Mobile[Mobile App<br/>React Native<br/>iOS/Android]:::frontend
        Web[Web Portal<br/>React + WebSockets<br/>Cliente/Operador]:::frontend
    end

    %% ============================================
    %% CAPA 3: API GATEWAY
    %% ============================================
    subgraph EDGE["üö™ API GATEWAY LAYER"]
        APIGateway[API Gateway<br/>Kong + OAuth2<br/>Auth, Rate Limit, Circuit Breaker]:::gateway
        LegacyProxy[Legacy Facade<br/>Spring Boot<br/>Anti-Corruption Layer]:::gateway
    end

    %% ============================================
    %% CAPA 4: SERVICIOS CORE
    %% ============================================
    subgraph CORE["‚≠ê CORE DOMAIN SERVICES"]
        Payment[Payment Service<br/>Spring WebFlux + R2DBC<br/>Orquestaci√≥n Pagos]:::core
        Ledger[Ledger Service<br/>Spring WebFlux<br/>Doble Partida Inmutable]:::core
        Fraud[Fraud Engine<br/>WebFlux + TensorFlow<br/>ML Scoring Tiempo Real]:::core
    end

    %% ============================================
    %% CAPA 5: SERVICIOS SOPORTE
    %% ============================================
    subgraph SUPPORT["üîß SUPPORTING SERVICES"]
        CustomerSvc[Customer Service<br/>Spring Boot<br/>Onboarding, KYC]:::support
        FX[FX Service<br/>Spring WebFlux<br/>Trading Divisas]:::support
        Clearing[Clearing Service<br/>Spring Boot<br/>Gateway SWIFT/PIX]:::support
        Notification[Notification Service<br/>Spring WebFlux<br/>Push/Email/SMS]:::support
        Reporting[Regulatory Reporting<br/>Spring Batch<br/>Compliance]:::support
        Reconciliation[Reconciliation Service<br/>Spring Batch<br/>Conciliaci√≥n Nocturna]:::support
    end

    %% ============================================
    %% CAPA 6: INTEGRACION
    %% ============================================
    subgraph INTEGRATION["üîå INTEGRATION LAYER"]
        Saga[Saga Orchestrator<br/>Temporal.io<br/>Transacciones Distribuidas]:::integration
        CDC[CDC Adapter<br/>Debezium<br/>Sync Bidireccional Legacy]:::integration
        HSMProxy[HSM Proxy<br/>Netty<br/>Firma Criptogr√°fica]:::integration
    end

    %% ============================================
    %% CAPA 7: DATOS
    %% ============================================
    subgraph DATA["üíæ DATA LAYER"]
        Kafka[(Event Bus<br/>Apache Kafka<br/>Log Eventos Dominio)]:::data
        Cache[(Cache<br/>Redis Cluster<br/>Sesiones, FX Rates, Tokens)]:::data
        PaymentDB[(Payment DB<br/>PostgreSQL<br/>Estado Transaccional)]:::data
        LedgerDB[(Ledger DB<br/>PostgreSQL + TimescaleDB<br/>Append-Only Ledger)]:::data
        FraudDB[(Fraud Store<br/>Cassandra<br/>Eventos Scoring ML)]:::data
        CustomerDB[(Customer DB<br/>PostgreSQL<br/>Maestro Clientes)]:::data
        DocsStore[(Document Store<br/>AWS S3<br/>KYC Docs, Extractos)]:::data
    end

    %% ============================================
    %% CAPA 8: OBSERVABILIDAD
    %% ============================================
    subgraph OBSERVABILITY["üìä OBSERVABILITY"]
        Tracing[Distributed Tracing<br/>Jaeger<br/>Spans End-to-End]:::observability
        Metrics[Metrics<br/>Prometheus + Grafana<br/>SLIs/SLOs]:::observability
        Logs[Centralized Logs<br/>ELK Stack<br/>Logs Agregados]:::observability
    end

    %% ============================================
    %% CAPA 9: SISTEMAS EXTERNOS
    %% ============================================
    subgraph EXTERNAL["üåê SISTEMAS EXTERNOS"]
        Legacy[LegacyCore<br/>Monolito Java 8<br/>Oracle PL/SQL]:::external
        SWIFT[SWIFT/SEPA/PIX<br/>Redes Compensaci√≥n]:::external
        Visa[Visa/Mastercard<br/>ISO 8583]:::external
        KYC[KYC Providers<br/>Jumio/Onfido<br/>Webhooks]:::external
        HSM[HSM F√≠sico<br/>Thales/nCipher<br/>On-Premise]:::external
        FCM[Firebase Cloud Messaging<br/>Push Notifications]:::external
    end

    %% ============================================
    %% RELACIONES: ACTORES ‚Üí CANALES
    %% ============================================
    Customer -->|HTTPS| Mobile
    Customer -->|HTTPS| Web
    Operator -->|HTTPS| Web

    %% ============================================
    %% RELACIONES: CANALES ‚Üí GATEWAY
    %% ============================================
    Mobile -->|REST API<br/>JSON/HTTPS| APIGateway
    Web -->|REST + WebSocket<br/>JSON/WSS| APIGateway

    %% ============================================
    %% RELACIONES: GATEWAY ‚Üí SERVICIOS
    %% ============================================
    APIGateway -->|Routes<br/>HTTP/2| Payment
    APIGateway -->|Routes<br/>HTTP/2| CustomerSvc
    APIGateway -->|Fallback<br/>HTTP| LegacyProxy
    APIGateway -.->|Valida Tokens<br/>OAuth2| Cache

    %% ============================================
    %% RELACIONES: CORE SERVICES ‚Üí KAFKA
    %% ============================================
    Payment <-->|Pub/Sub<br/>PaymentEvents| Kafka
    Ledger <-->|Pub/Sub<br/>LedgerEvents| Kafka
    Fraud <-->|Consume/Produce<br/>FraudScore| Kafka

    %% ============================================
    %% RELACIONES: SUPPORT SERVICES ‚Üí KAFKA
    %% ============================================
    Notification -->|Consume<br/>StatusChanged| Kafka
    Reporting -->|Consume<br/>Audit Trail| Kafka

    %% ============================================
    %% RELACIONES: SAGA ORCHESTRATION
    %% ============================================
    Saga -.->|Coordina<br/>gRPC| Payment
    Saga -.->|Coordina<br/>gRPC| Ledger
    Saga -.->|Coordina<br/>gRPC| Clearing

    %% ============================================
    %% RELACIONES: SERVICIOS ‚Üí DATABASES
    %% ============================================
    Payment -->|R/W<br/>R2DBC| PaymentDB
    Ledger -->|Append-Only<br/>JDBC| LedgerDB
    Fraud -->|Escritura Masiva<br/>CQL| FraudDB
    CustomerSvc -->|R/W<br/>JDBC| CustomerDB

    %% ============================================
    %% RELACIONES: SERVICIOS ‚Üí CACHE
    %% ============================================
    Payment -.->|Cache Saldos<br/>Redis Protocol| Cache
    FX -.->|Cache Rates 5min<br/>Redis Protocol| Cache

    %% ============================================
    %% RELACIONES: SEGURIDAD HSM
    %% ============================================
    Payment -->|Firma TX<br/>TLS Mutual| HSMProxy
    HSMProxy -->|PKCS#11<br/>VPN Dedicada| HSM

    %% ============================================
    %% RELACIONES: CLEARING ‚Üí REDES EXTERNAS
    %% ============================================
    Clearing -->|ISO 20022 XML<br/>SWIFT Alliance| SWIFT
    Clearing -->|ISO 8583<br/>TCP Persistente| Visa

    %% ============================================
    %% RELACIONES: CUSTOMER ‚Üí KYC
    %% ============================================
    CustomerSvc -->|POST /verify<br/>Webhook| KYC
    KYC -.->|Callback<br/>Async| CustomerSvc
    CustomerSvc -->|Upload IDs<br/>S3 API| DocsStore

    %% ============================================
    %% RELACIONES: NOTIFICACIONES
    %% ============================================
    Notification -->|Env√≠a Push<br/>FCM API| FCM

    %% ============================================
    %% RELACIONES: CDC ‚Üí LEGACY
    %% ============================================
    CDC <-->|Change Data Capture<br/>Oracle GoldenGate| Legacy
    CDC -->|Publica ChangeEvents<br/>Kafka Protocol| Kafka
    LegacyProxy -->|SOAP/REST<br/>Anti-Corruption| Legacy

    %% ============================================
    %% RELACIONES: RECONCILIACION
    %% ============================================
    Reconciliation -->|Lee Movimientos<br/>SQL| LedgerDB
    Reconciliation -->|Descarga MT940<br/>S3 API| DocsStore

    %% ============================================
    %% RELACIONES: OBSERVABILIDAD
    %% ============================================
    Payment -.->|Spans<br/>OpenTelemetry| Tracing
    Fraud -.->|M√©tricas ML<br/>Prometheus| Metrics
    Ledger -.->|Structured Logs<br/>Fluentd| Logs

    %% ============================================
    %% RELACIONES: REPORTING
    %% ============================================
    Regulator -->|Consulta Reportes<br/>SFTP| Reporting
```

---

## üìä Inventario de Contenedores

### Frontend Applications

| Contenedor | Tecnolog√≠a | Responsabilidad |
|------------|------------|------------------|
| **Mobile App** | React Native + TypeScript | UI para clientes (pagos, saldo, historial) |
| **Web Portal** | React + TypeScript + WebSockets | Portal unificado para clientes y operadores |

**Caracter√≠sticas**:
- Autenticaci√≥n: OAuth2 + biometr√≠a (Touch ID)
- Offline-first (cache local con sync)
- Push notifications (Firebase Cloud Messaging)

---

### Backend Services (Microservices)

#### üî¥ Core Domain Services

| Servicio | Tecnolog√≠a | Bounded Context | Base de Datos | Criticidad |
|----------|------------|-----------------|---------------|------------|
| **Payment Service** | Spring Boot 3 + WebFlux + R2DBC | Payment Processing | PostgreSQL | üî¥ Cr√≠tico |
| **Ledger Service** | Spring Boot 3 + WebFlux | General Ledger | PostgreSQL + TimescaleDB (append-only) | üî¥ Cr√≠tico |
| **Fraud Service** | Spring Boot 3 + WebFlux + TensorFlow | Fraud Management | Cassandra + Redis (feature store) | üî¥ Cr√≠tico |

##### Payment Service

```yaml
Responsabilidades:
  - Crear y validar √≥rdenes de pago
  - Gestionar ciclo de vida (Draft ‚Üí Validated ‚Üí Sent ‚Üí Settled)
  - Orquestar validaciones (saldo, fraude, compliance)
  - Publicar eventos: PaymentCreated, PaymentExecuted, PaymentFailed

APIs Expuestas:
  - POST /api/v1/payments (crear pago)
  - GET /api/v1/payments/{id} (consultar estado)
  - GET /api/v1/payments?customerId=X (historial)

Eventos Consumidos:
  - FraudCheckCompleted (de Fraud Service)
  - LedgerEntryCreated (de Ledger Service)

Eventos Publicados:
  - PaymentCreated
  - PaymentExecuted
  - PaymentFailed

M√©tricas Clave:
  - payment_creation_rate (TPS)
  - payment_processing_latency_p99 (ms)
  - payment_failure_rate (%)
```

##### Ledger Service

```yaml
Responsabilidades:
  - Registro inmutable de doble entrada (Event Sourcing)
  - Proyecci√≥n de saldos en read models
  - Garantizar consistencia transaccional
  - Append-only ledger con TimescaleDB para time-series

Patr√≥n de Arquitectura:
  - CQRS (Command/Query Responsibility Segregation)
  - Event Sourcing (Kafka como event log)
  - TimescaleDB para queries temporales eficientes

Comandos (Write):
  - CreateLedgerEntry(debit, credit, amount, timestamp)

Queries (Read):
  - GetBalance(accountId)
  - GetTransactionHistory(accountId, dateRange)
  - GetBalanceAt(accountId, timestamp) - Time travel queries

Event Store:
  - Kafka topic: ledger-events (particionado por accountId)
  - Retention: Infinita (compliance PCI-DSS/GDPR)

Read Models:
  - PostgreSQL + TimescaleDB (saldos consolidados, hypertables)
  - Proyecciones con lag < 1s (consistencia eventual)
```

##### Fraud Service

```yaml
Responsabilidades:
  - Scoring de riesgo en tiempo real (< 50ms)
  - Evaluaci√≥n de reglas de negocio (12 reglas din√°micas)
  - Entrenamiento de modelos ML (offline)
  - Almacenamiento masivo de eventos de scoring

Arquitectura ML:
  - Feature Store: Redis (velocidad transaccional, geo, device fingerprint)
  - Model Serving: TensorFlow Serving / ONNX Runtime
  - Event Storage: Cassandra (escritura masiva > 10K writes/s)
  - Retraining: Batch diario con Spark

Pipeline:
  1. PaymentCreated ‚Üí Feature Engineering (extrae 50+ features)
  2. Model Inference (score 0-100, latencia < 30ms)
  3. Rule Evaluation (12 reglas: velocity, geolocation, blacklist)
  4. Decisi√≥n: APPROVED / REVIEW / BLOCKED
  5. Escritura evento en Cassandra (async, no bloquea)

M√©tricas:
  - fraud_scoring_latency_p99 (< 50ms)
  - false_positive_rate (< 5%)
  - fraud_detection_rate (> 95%)
  - cassandra_write_throughput (> 10K/s)
```

---

#### üü° Supporting Domain Services

| Servicio | Tecnolog√≠a | Responsabilidad | Criticidad |
|----------|------------|-----------------|------------|
| **Customer Service** | Spring Boot (JDBC, no reactivo por KYC) | Onboarding, KYC, gesti√≥n de clientes | üü° Alto |
| **FX Service** | Spring Boot 3 + WebFlux | Pricing FX, bloqueo de tasas (5 min) | üü° Alto |
| **Clearing Service** | Spring Boot (manejo de sockets SWIFT/ISO 8583) | Integraci√≥n con SWIFT, SEPA, PIX, Visa/MC | üî¥ Cr√≠tico |
| **Notification Service** | Spring Boot 3 + WebFlux | Push (FCM), Email, SMS | üü¢ Medio |
| **Regulatory Reporting** | Spring Batch | Reportes de cumplimiento para reguladores | üü° Alto |
| **Reconciliation Service** | Spring Batch | Reconciliaci√≥n nocturna con extractos MT940 | üü° Alto |

##### FX Service

```yaml
Responsabilidades:
  - Obtener tasas de cambio en tiempo real
  - Bloquear tasas (5 minutos)
  - Aplicar spread comercial

Integraci√≥n con Proveedores:
  - Reuters: WebSocket streaming (tasas spot)
  - Bloomberg: REST API fallback

Caching:
  - Redis TTL=10s (tasas por par de divisas)
  - Invalidaci√≥n: Push desde provider

APIs:
  - GET /api/v1/fx/rates?from=USD&to=EUR
  - POST /api/v1/fx/locks (bloquear tasa)
  - GET /api/v1/fx/locks/{lockId} (verificar expiraci√≥n)
```

##### Clearing Service

```yaml
Responsabilidades:
  - Adaptar modelo interno ‚Üí protocolos externos
  - Gestionar conexiones stateful (SWIFT TCP)
  - Reintentos y manejo de errores

Adaptadores (Bridge Pattern):
  - SWIFT Adapter: ISO 20022 XML ‚Üí TCP socket
  - SEPA Adapter: CAMT.053 XML ‚Üí HTTPS
  - PIX Adapter: JSON ‚Üí REST API
  - ACH Adapter: Flat File ‚Üí SFTP

Resiliencia:
  - Circuit Breaker: Si SWIFT timeout ‚Üí reintentar 3x
  - Fallback: Si red primaria cae ‚Üí red alternativa
  - Dead Letter Queue: Mensajes fallidos ‚Üí manual review
```

---

#### üü¢ Generic Domain Services

| Servicio | Tecnolog√≠a | Responsabilidad | Estrategia |
|----------|------------|-----------------|------------|
| **Notification Service** | Spring Boot 3 + WebFlux | Push, Email, SMS | Event-Driven, SaaS providers |

```yaml
Notification Service:
  Responsabilidades:
    - Consumir eventos de negocio
    - Renderizar templates
    - Enviar por canal adecuado
  
  Proveedores:
    - Push: Firebase Cloud Messaging
    - Email: SendGrid
    - SMS: Twilio
  
  Eventos Consumidos:
    - PaymentExecuted ‚Üí "Tu pago fue enviado"
    - PaymentFailed ‚Üí "Tu pago fall√≥: {reason}"
    - KYCVerified ‚Üí "Tu cuenta fue activada"
  
  Patr√≥n:
    - Observer Pattern
    - Template Method (renderizado)
```

---

#### üîå Integration Layer (Strangler Fig Migration)

| Servicio | Tecnolog√≠a | Responsabilidad | Criticidad |
|----------|------------|-----------------|------------|
| **Saga Orchestrator** | Temporal.io | Transacciones distribuidas con compensaci√≥n | üî¥ Cr√≠tico |
| **CDC Adapter** | Debezium + Oracle GoldenGate | Change Data Capture para sincronizaci√≥n Legacy ‚Üî Nuevo | üü° Alto |
| **HSM Proxy** | Netty (non-blocking I/O) | Firma criptogr√°fica con HSM f√≠sico on-premise | üî¥ Cr√≠tico |
| **Legacy Facade** | Spring Boot (ACL) | Anti-Corruption Layer para Legacy Monolith | üî¥ Cr√≠tico |

##### Saga Orchestrator (Temporal.io)

```yaml
Responsabilidades:
  - Coordinar transacciones distribuidas multi-servicio
  - Implementar compensaci√≥n autom√°tica (Saga Pattern)
  - Manejar timeouts, retries, y rollbacks
  - Garantizar consistencia eventual

Workflows Implementados:
  - ProcessPaymentSaga:
      Pasos: Payment ‚Üí Fraud Check ‚Üí Ledger Entry ‚Üí Clearing
      Compensaci√≥n: Si Clearing falla ‚Üí reversar Ledger ‚Üí reembolsar Payment
  
  - RefundSaga:
      Pasos: Reverse Ledger ‚Üí Update Payment Status ‚Üí Notify Customer
  
  - OnboardingSaga:
      Pasos: Create Customer ‚Üí KYC Verification ‚Üí Activate Account ‚Üí Send Welcome

Tecnolog√≠a:
  - Temporal.io: Workflow engine durable (state persistido en PostgreSQL)
  - Comunicaci√≥n: gRPC con servicios
  
Resiliencia:
  - Retry autom√°tico con backoff exponencial
  - Timeout configurable por paso (default 30s)
  - Compensating transactions si falla cualquier paso
  - Idempotencia garantizada (mismo input ‚Üí mismo resultado)

Justificaci√≥n vs alternativas:
  - ‚ùå Saga manual (c√≥digo custom): Complejo, error-prone
  - ‚ùå 2PC (Two-Phase Commit): No escala, no soporta fallos parciales
  - ‚úÖ Temporal.io: Workflow as code, retry built-in, observabilidad
```

##### CDC Adapter (Debezium + Oracle GoldenGate)

```yaml
Responsabilidades:
  - Capturar cambios en Legacy Oracle Database (Change Data Capture)
  - Publicar eventos al Event Bus (Kafka) en tiempo real
  - Sincronizaci√≥n bidireccional Legacy ‚Üî Nuevo Sistema
  - Evitar dual-writes inconsistentes durante Strangler Fig

Arquitectura:
  - Debezium Connector para Oracle: Lee WAL (Write-Ahead Log)
  - Oracle GoldenGate: Captura cambios en tablas cr√≠ticas
  - Kafka Connect: Publica eventos a Kafka topics

Flujo de Sincronizaci√≥n:
  1. Legacy ejecuta: UPDATE CUSTOMERS SET email='...' WHERE id=123
  2. GoldenGate captura cambio en WAL
  3. Debezium publica evento: CustomerUpdated {id: 123, email: '...'}
  4. Customer Service (nuevo) consume evento
  5. Customer Service actualiza su propia BD (customer_db PostgreSQL)

Tablas Sincronizadas (Legacy ‚Üí Nuevo):
  - CUSTOMERS ‚Üí Customer Service
  - ACCOUNTS ‚Üí Ledger Service (solo lectura)
  - TRANSACTIONS ‚Üí Payment Service (hasta 100% migrado)

Justificaci√≥n:
  - Problema kata: "Sistemas sat√©lites leen/escriben directo en BD Legacy"
  - Soluci√≥n: CDC elimina shared database, publica eventos a Kafka
  - Durante Strangler Fig: Ambos sistemas coexisten sin dual-writes
  
Trade-offs:
  - ‚úÖ Pro: Consistencia eventual < 5s, no requiere cambios en Legacy
  - ‚ùå Con: Complejidad operacional (Debezium/GoldenGate), lag eventual
```

##### HSM Proxy (Netty)

```yaml
Responsabilidades:
  - Abstraer comunicaci√≥n con HSM f√≠sico on-premise
  - Firmar transacciones cr√≠ticas (pagos > $10K, transferencias internacionales)
  - Manejar PKCS#11 protocol
  - Connection pooling a HSM (recurso escaso)

Arquitectura:
  - Netty: non-blocking I/O para alta concurrencia
  - TLS Mutual Authentication
  - VPN dedicada con datacenter on-premise

Flujo:
  1. Payment Service crea pago internacional > $10K
  2. Payment ‚Üí HSM Proxy: POST /sign {txId, amount, hash}
  3. HSM Proxy ‚Üí HSM F√≠sico: PKCS#11 SignData
  4. HSM responde con firma digital
  5. HSM Proxy ‚Üí Payment: {txId, signature}
  6. Payment incluye signature en mensaje SWIFT

Resiliencia:
  - Connection pool: 10 conexiones persistentes al HSM
  - Timeout: 100ms (fail-fast si HSM no responde)
  - Circuit breaker: Si 50% fallos en 10s ‚Üí abrir circuito, rechazar pagos
  - Fallback: Pagos < $10K no requieren HSM (solo TLS cert)

Justificaci√≥n:
  - Problema kata: "HSMs f√≠sicos on-premise, migraci√≥n a Cloud"
  - Soluci√≥n: HSM Proxy permite mantener HSM f√≠sico, abstraer para microservicios
  - Futuro: Migrar a AWS CloudHSM sin cambiar Payment Service
```

##### Legacy Facade (Anti-Corruption Layer)

```yaml
Responsabilidades:
  - Adaptar APIs del Legacy Monolith (SOAP/REST legacy) a APIs modernas
  - Traducir modelo de datos Legacy ‚Üí modelo de dominio nuevo
  - Evitar que l√≥gica legacy "contamine" nuevos servicios
  - Routing durante Strangler Fig (30% nuevo, 70% legacy)

Patr√≥n DDD:
  - Anti-Corruption Layer (ACL)
  - Protege bounded contexts nuevos de dependencias Legacy

APIs Expuestas (para nuevos servicios):
  - GET /legacy/customers/{id} ‚Üí traduce de Oracle CUSTOMERS table
  - GET /legacy/balance/{accountId} ‚Üí consulta saldos en Legacy
  - POST /legacy/transactions ‚Üí crea transacci√≥n en Legacy (fallback)

Adaptaciones:
  - Legacy usa codes num√©ricos (status: 1, 2, 3, 9)
  - Facade traduce a enums: DRAFT, VALIDATED, SENT, FAILED
  
  - Legacy usa CUSTOMER_TYPE: 'P' (Persona), 'C' (Corporativo)
  - Facade traduce a: CustomerType.INDIVIDUAL, CustomerType.BUSINESS

Uso en Strangler Fig:
  - API Gateway: Si Payment ya migrado ‚Üí Payment Service, sino ‚Üí Legacy Facade
  - Payment Service consulta saldos via Facade (no directo a Oracle)
  - Ledger Service sincroniza asientos via Facade

Tecnolog√≠a:
  - Spring Boot (no reactivo, ya que Legacy es bloqueante)
  - Circuit Breaker (Resilience4j)
  - Cache Redis (5 min TTL para consultas frecuentes)
```

---

### Data Stores

| Base de Datos | Tecnolog√≠a | Uso | Patr√≥n | Backup |
|---------------|------------|-----|--------|--------|
| **payment_db** | PostgreSQL 14 | Estado transaccional de pagos | Database per Service | Continuous WAL ‚Üí S3 |
| **ledger_db** | PostgreSQL 14 + TimescaleDB | Ledger append-only (doble partida inmutable) | Event Sourcing | Kafka retention infinita |
| **customer_db** | PostgreSQL 14 | Maestro de clientes, datos KYC | Database per Service | Daily snapshot |
| **fraud_store** | Cassandra | Eventos de scoring ML (escritura masiva) | Time-series DB | Daily snapshot |
| **cache** | Redis Cluster | Sesiones, FX rates, tokens OAuth2, feature store ML | Shared Cache | No backup (ephemeral) |
| **document_store** | AWS S3 | KYC docs (selfies, IDs), extractos bancarios MT940 | Object Storage | Multi-region replication |

---

### Messaging & Event Streaming

#### Apache Kafka

```yaml
Clusters:
  - kafka-events: Eventos de dominio (PaymentExecuted, etc.)
  - kafka-ledger: Event Store para Ledger (retention infinita)

Topics Principales:
  payment-events:
    particiones: 12 (por regi√≥n geogr√°fica)
    retention: 30 d√≠as
    consumidores: Ledger, Fraud, Notification
  
  ledger-events:
    particiones: 20 (por accountId hash)
    retention: Infinita (compliance)
    consumidores: Read Model Projectors, Reconciliation
  
  fraud-events:
    particiones: 8
    retention: 90 d√≠as
    consumidores: Analytics, ML Retraining Pipeline

Garant√≠as:
  - At-least-once delivery
  - Order garantizado por partici√≥n
  - Compactaci√≥n para read models
```

---

### Observability Stack

| Componente | Tecnolog√≠a | Uso |
|------------|------------|-----|
| **Monitoring** | Prometheus + Grafana | M√©tricas de negocio (TPS, latencia) y t√©cnicas (CPU, RAM) |
| **Tracing** | Jaeger + OpenTelemetry | Distributed tracing end-to-end |
| **Logging** | ELK Stack | Logs centralizados, b√∫squeda, alertas |

```yaml
M√©tricas Clave (Prometheus):
  - payment_tps (Gauge): Transacciones por segundo
  - payment_latency_p99 (Histogram): Latencia P99
  - fraud_score_distribution (Histogram): Distribuci√≥n de scores
  - ledger_consistency_lag (Gauge): Lag de consistencia eventual
  - api_error_rate (Counter): Errores HTTP 5xx

Trazas (Jaeger):
  - Span ra√≠z: POST /api/v1/payments
  - Sub-spans: Fraud Check, Ledger Entry, SWIFT Message
  - Context propagation: TraceId en headers HTTP + Kafka

Logs (ELK):
  - Formato: JSON estructurado
  - Campos: timestamp, service, traceId, level, message, context
  - Retention: 30 d√≠as (hot) + 1 a√±o (cold en S3)
```

---

## üîó Patrones de Comunicaci√≥n

### S√≠ncrona (Request-Response)

```
Mobile App ‚Üí API Gateway ‚Üí Payment Service
  ‚Üì (HTTPS/REST)
  ‚Üì Respuesta inmediata (200 OK) con ID de pago
  ‚Üì Estado inicial: DRAFT
```

**Cu√°ndo usar**: Queries (GET), comandos que requieren respuesta inmediata (< 200ms).

---

### As√≠ncrona (Event-Driven)

```
Payment Service ‚Üí Kafka (PaymentExecuted event)
  ‚Üì (Pub/Sub)
  ‚îú‚Üí Ledger Service (crea LedgerEntry)
  ‚îú‚Üí Notification Service (env√≠a Push)
  ‚îî‚Üí Analytics Service (actualiza dashboard)
```

**Cu√°ndo usar**: Procesos que pueden ser eventuales, comunicaci√≥n entre bounded contexts.

---

## üîç Decisiones Arquitect√≥nicas Clave

Esta secci√≥n documenta las decisiones t√©cnicas cr√≠ticas tomadas para la arquitectura de contenedores de FinScale GlobalLedger, justificando cada elecci√≥n desde el valor de negocio y los drivers arquitect√≥nicos.

---

### 1. Saga Orchestrator: Temporal.io

**Contexto del Problema:**
Las transacciones distribuidas multi-servicio (Payment ‚Üí Fraud ‚Üí Ledger ‚Üí Clearing) requieren coordinaci√≥n compleja con compensaci√≥n autom√°tica. En un sistema con 1M TPS, una falla parcial (ej. Clearing falla despu√©s de d√©bito en Ledger) debe revertirse autom√°ticamente sin intervenci√≥n manual.

**Decisi√≥n:** Temporal.io como motor de orquestaci√≥n de Sagas

**Justificaci√≥n:**
- **Durabilidad del Estado**: Workflows persisten en base de datos. Si el orquestador falla, retoma desde el √∫ltimo checkpoint sin perder contexto.
- **Compensaci√≥n Autom√°tica**: C√≥digo declarativo define pasos y compensaciones. Temporal garantiza ejecuci√≥n de rollback en orden inverso.
- **Retry Inteligente**: Backoff exponencial configurable por paso (ej. SWIFT puede tardar 4 horas, Temporal espera sin bloquear threads).
- **Observabilidad Built-in**: UI web muestra estado de cada transacci√≥n en tiempo real, cr√≠tico para operadores de FinScale.

**Alternativas Evaluadas:**
- Implementaci√≥n manual con Spring State Machine: Mayor control pero requiere c√≥digo custom para retry, compensaci√≥n, persistencia de estado ‚Üí alto riesgo de bugs en l√≥gica cr√≠tica.
- Choreography pura (eventos sin orquestador): M√°s resiliente pero dificulta rastrear sagas complejas de 5+ pasos ‚Üí incumple requisito de trazabilidad total (PCI-DSS).

**Trade-offs Aceptados:**
- ‚úÖ Robustez y observabilidad
- ‚ùå Dependencia adicional (Temporal cluster)
- ‚ùå Curva de aprendizaje del equipo (mitigado con training)

**Impacto en Drivers:**
- **Time-to-Market**: Reduce de 4 meses a 6 semanas implementar nueva saga (ej. agregar pa√≠s).
- **Disponibilidad 99.999%**: Temporal reinicia workflows autom√°ticamente tras fallos.

---

### 2. Change Data Capture: Debezium + Oracle GoldenGate

**Contexto del Problema:**
Durante la migraci√≥n Strangler Fig, el Legacy Monolith y los nuevos microservicios deben coexistir. Sistemas sat√©lites (Reportes, CRM legacy) leen/escriben directamente en tablas de Oracle. Cambiar el esquema rompe integraciones desconocidas. Dual-writes manuales causan inconsistencias.

**Decisi√≥n:** CDC con Debezium connector para Oracle + GoldenGate

**Justificaci√≥n:**
- **Sincronizaci√≥n en Tiempo Real**: Captura cambios en Write-Ahead Log de Oracle (lag < 5s) y publica eventos a Kafka. Customer Service (nuevo) se mantiene sincronizado sin modificar Legacy.
- **Zero-Downtime Migration**: Ambos sistemas operan en paralelo. Legacy sigue siendo source of truth hasta que el nuevo servicio alcanza 100% de tr√°fico.
- **No Requiere Cambios en Legacy**: GoldenGate lee WAL sin tocar c√≥digo Java 8 ni PL/SQL (40% del c√≥digo cr√≠tico).
- **Trazabilidad**: Cada cambio genera evento con timestamp, usuario, tabla ‚Üí cumple GDPR data lineage.

**Impacto en Deuda T√©cnica:**
Resuelve problema espec√≠fico de "Integraci√≥n por Base de Datos Compartida": sistemas sat√©lites migran a consumir eventos de Kafka en lugar de queries SQL directos.

**Trade-offs Aceptados:**
- ‚úÖ Migraci√≥n incremental sin riesgo
- ‚úÖ No requiere refactor de Legacy (40% PL/SQL intocable)
- ‚ùå Consistencia eventual (lag 5s m√°ximo)
- ‚ùå Infraestructura adicional (Debezium cluster, GoldenGate licenses)

**Impacto en Drivers:**
- **Resiliencia**: Si falla CDC, servicios nuevos siguen operando con datos en cache (degradaci√≥n elegante).
- **Modernizaci√≥n**: Permite migrar bounded context por bounded context sin "Big Bang".

---

### 3. Fraud Storage: Apache Cassandra

**Contexto del Problema:**
Fraud Service genera 50,000+ eventos de scoring por segundo (cada PaymentCreated ‚Üí scoring + 12 reglas). PostgreSQL con escritura s√≠ncrona causa contenci√≥n de locks bajo carga extrema (1M TPS objetivo). Fraud scoring debe ser < 50ms end-to-end.

**Decisi√≥n:** Apache Cassandra para almacenamiento de eventos de fraude

**Justificaci√≥n:**
- **Escritura Masiva Optimizada**: Cassandra est√° dise√±ado para writes (no reads). LSM-tree permite 50K+ writes/s sin degradaci√≥n.
- **Escalamiento Horizontal Lineal**: Agregar nodos aumenta throughput linealmente (vs PostgreSQL vertical scaling limitado).
- **Time-Series Natural**: Partition key por timestamp permite queries eficientes de eventos recientes para retraining ML.
- **No Bloquea Payment Pipeline**: Escritura as√≠ncrona despu√©s de scoring ‚Üí si Cassandra falla, scoring sigue funcionando (eventos se pierden pero pago NO se bloquea).

**Por qu√© NO PostgreSQL:**
- PostgreSQL requiere VACUUM peri√≥dico ‚Üí pausas de 10-30s cada hora bajo escritura masiva.
- Contenci√≥n de locks en tabla de 500M+ rows (6 meses de eventos).

**Trade-offs Aceptados:**
- ‚úÖ Throughput 10x superior a PostgreSQL
- ‚úÖ No bloquea path cr√≠tico (Payment execution)
- ‚ùå Eventual consistency (no ACID completo)
- ‚ùå Operaci√≥n m√°s compleja (tuning de compaction)

**Impacto en Drivers:**
- **Escalabilidad 1M TPS**: Cassandra maneja escritura sin ser cuello de botella.
- **Resiliencia**: Fraud storage separado ‚Üí falla no afecta Payment/Ledger.

---

### 4. Ledger Time-Series: PostgreSQL + TimescaleDB Extension

**Contexto del Problema:**
Ledger requiere queries time-travel ("¬øcu√°l era el saldo de cuenta X el 15 de marzo?") para auditor√≠as y disputas. Event Sourcing genera 2M+ eventos/d√≠a. Queries de agregaci√≥n sobre ventanas temporales (ej. "√∫ltimos 30 d√≠as") son lentos en PostgreSQL puro.

**Decisi√≥n:** TimescaleDB extension sobre PostgreSQL

**Justificaci√≥n:**
- **Hypertables**: Particiona autom√°ticamente por timestamp sin cambios en queries SQL.
- **Compresi√≥n Columnar**: Eventos antiguos (> 90 d√≠as) se comprimen 20:1 ‚Üí reduce costos de storage.
- **Time-Bucket Aggregations**: Queries como `SELECT time_bucket('1 day', timestamp), SUM(amount)` son 50x m√°s r√°pidos que PostgreSQL est√°ndar.
- **Compatibilidad PostgreSQL**: No es una BD nueva ‚Üí mismo ecosistema (R2DBC, JDBC, pg_dump backups).

**Por qu√© NO PostgreSQL Puro:**
- Particionamiento manual en PostgreSQL requiere DDL por mes/a√±o.
- Sin optimizaci√≥n para queries temporales ‚Üí full table scans en auditor√≠as.

**Por qu√© NO Elasticsearch:**
- Elasticsearch es eventually consistent ‚Üí Ledger requiere consistencia fuerte.
- No soporta transacciones ACID.

**Trade-offs Aceptados:**
- ‚úÖ Queries temporales 50x m√°s r√°pidos
- ‚úÖ Retenci√≥n infinita con compresi√≥n (compliance)
- ‚ùå Extension adicional (pero 100% compatible con PostgreSQL)

**Impacto en Drivers:**
- **Cumplimiento PCI-DSS**: Audit trail con time-travel queries (requerido por reguladores).
- **Disponibilidad 24/7**: Compresi√≥n autom√°tica ‚Üí sin ventanas de mantenimiento.

---

### 5. HSM Integration: Netty-based Proxy

**Contexto del Problema:**
FinScale usa HSMs f√≠sicos on-premise (Thales) para firmar transacciones > $10K (cumplimiento PCI-DSS). HSMs usan protocolo PKCS#11 sobre conexi√≥n TCP persistente. Microservicios en Kubernetes son ef√≠meros y stateless.

**Decisi√≥n:** HSM Proxy con Netty (non-blocking I/O)

**Justificaci√≥n:**
- **Connection Pooling Persistente**: HSMs limitan a 50 conexiones simult√°neas. Netty mantiene pool de 10 conexiones reutilizables (vs crear/destruir por request).
- **Non-blocking I/O**: Netty maneja 10K+ requests concurrentes sin threads bloqueados (vs Spring Boot bloqueante que requiere thread pool masivo).
- **Abstracci√≥n de HSM**: Payment Service llama REST API simple ‚Üí HSM Proxy maneja complejidad de PKCS#11, VPN, mutual TLS.
- **Migraci√≥n Futura**: Cuando FinScale migre a AWS CloudHSM, solo se cambia implementaci√≥n del Proxy (Payment Service no se toca).

**Por qu√© NO Spring Boot Est√°ndar:**
- Spring Boot MVC es bloqueante ‚Üí cada request bloquea un thread esperando HSM (latencia 50-100ms).
- Con 1M TPS, necesitar√≠a thread pool de 100K threads ‚Üí OOM.

**Trade-offs Aceptados:**
- ‚úÖ Latencia < 100ms para firma
- ‚úÖ Abstracci√≥n permite migrar a CloudHSM sin cambios en servicios
- ‚ùå Componente custom (Netty) ‚Üí requiere expertise espec√≠fico

**Impacto en Drivers:**
- **Escalabilidad 1M TPS**: Non-blocking permite manejar carga sin crear miles de threads.
- **Modernizaci√≥n**: Facilita migraci√≥n futura a Cloud HSM.

---

### 6. API Gateway: Kong + OAuth2

**Contexto del Problema:**
Durante Strangler Fig, el API Gateway debe enrutar progresivamente (30% nuevo, 70% legacy). Sticky Sessions actuales (HttpSession en RAM) impiden escalamiento horizontal. Requiere autenticaci√≥n centralizada compatible con Legacy y nuevos servicios.

**Decisi√≥n:** Kong API Gateway con OAuth2 + Redis session store

**Justificaci√≥n:**
- **Routing Din√°mico**: Configuraci√≥n declarativa (YAML) define % de tr√°fico por ruta ‚Üí cambiar de 30% a 50% sin deploy.
- **Externalizaci√≥n de Sesiones**: Redis cluster reemplaza HttpSession ‚Üí si pod de Kong muere, sesi√≥n persiste.
- **Rate Limiting Centralizado**: Previene abuse (ej. 100 req/min por usuario) ‚Üí protege Legacy durante migraci√≥n.
- **Circuit Breaker**: Si Legacy responde > 5s, Kong abre circuito y devuelve cached response o error graceful.

**Impacto en Deuda T√©cnica:**
Resuelve problema espec√≠fico de "Sticky Sessions": Redis session store permite escalamiento horizontal inmediato.

**Trade-offs Aceptados:**
- ‚úÖ Strangler Fig sin downtime
- ‚úÖ Escalamiento horizontal de frontend
- ‚ùå Single Point of Failure (mitigado con Kong cluster Multi-AZ)

**Impacto en Drivers:**
- **Disponibilidad 99.999%**: Circuit breaker evita cascading failures.
- **Time-to-Market**: Migraci√≥n progresiva (30%‚Üí50%‚Üí100%) reduce riesgo.

---

## üéØ Drivers de Arquitectura Soportados (Nivel Contenedor)

| Driver | Decisi√≥n Arquitect√≥nica | Trade-off |
|--------|-------------------------|-----------|
| **Escalabilidad (1M TPS)** | Microservicios stateless + Kafka particionado | Complejidad operacional vs. throughput |
| **Disponibilidad (99.999%)** | Database per service + Multi-AZ deployment | Costo vs. resiliencia |
| **Resiliencia** | Event-Driven (desacoplamiento) + Circuit Breakers | Latencia eventual vs. acoplamiento |
| **Time-to-Market** | Bounded Contexts = Equipos aut√≥nomos | Overhead de coordinaci√≥n vs. velocidad |

---

**Pr√≥ximo Paso**: ‚Üí `C3-Componentes.md` para desglosar Payment Service internamente.

---

**√öltima actualizaci√≥n**: 24 de diciembre de 2025  
**Referencias**: [2.2-Bounded-Contexts.md](../../02-Dise√±o-Estrategico/2.2-Bounded-Contexts.md), [1.2-Drivers-Arquitectura.md](../../01-Entendimiento-Negocio/1.2-Drivers-Arquitectura.md)
