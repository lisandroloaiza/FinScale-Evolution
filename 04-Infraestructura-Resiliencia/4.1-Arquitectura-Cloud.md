# Arquitectura Cloud Multi-Region

> **Objetivo**: Dise√±ar infraestructura con 99.999% de disponibilidad  
> **Proveedor**: AWS (estrategia multi-cloud futura)

---

## üåç Vista Estrat√©gica Multi-AZ (Alto Nivel)

**Drivers de Arquitectura**: Disponibilidad 99.999%, Escalabilidad 2K‚Üí1M TPS

```mermaid
---
config:
  layout: elk
---
flowchart TB
 subgraph Global["üåê Capa Global"]
        Users["üë• Usuarios<br/>Mobile + Web<br/>12 pa√≠ses"]
        CDN["CloudFront CDN<br/>400+ Edge Locations<br/>Cache Hit Ratio: 85%"]
        DNS["Route53<br/>Geo DNS + Health Checks<br/>Failover &lt; 30s"]
  end
 subgraph RegionPrimary["‚òÅÔ∏è AWS Region: us-east-1 (Primary)"]
    direction TB
     subgraph AZ1["üìç AZ-A<br/>Capacity: 33%"]
            Compute1["Compute Layer<br/>27 EKS Nodes<br/>9 compute + 6 memory + 3 GPU<br/>Target: 333K TPS"]
            Data1["Data Layer<br/>PostgreSQL Primary<br/>Redis Primary<br/>Kafka Leader<br/>Cassandra + TimescaleDB"]
      end
     subgraph AZ2["üìç AZ-B<br/>Capacity: 33%"]
            Compute2["Compute Layer<br/>27 EKS Nodes<br/>9 compute + 6 memory + 3 GPU<br/>Target: 333K TPS"]
            Data2["Data Layer<br/>PostgreSQL Standby<br/>Redis Replica<br/>Kafka Follower<br/>Cassandra + TimescaleDB"]
      end
     subgraph AZ3["üìç AZ-C<br/>Capacity: 33%"]
            Compute3["Compute Layer<br/>27 EKS Nodes<br/>9 compute + 6 memory + 3 GPU<br/>Target: 333K TPS"]
            Data3["Data Layer<br/>Redis Replica<br/>Kafka Follower<br/>Cassandra + TimescaleDB"]
      end
     subgraph SharedSvcs["üîß Shared Services<br/>Cross-AZ"]
            Orchestration["Temporal.io<br/>Saga Orchestration<br/>5 pods distribuidos"]
            Search["Elasticsearch<br/>CQRS Read Model<br/>6 nodes distributed"]
            Monitoring["Observability<br/>Prometheus + Grafana<br/>Jaeger + ELK"]
      end
  end
 subgraph RegionDR["‚òÅÔ∏è AWS Region: us-west-2 (DR)<br/>üî¥ Pasivo - Activaci√≥n Manual"]
        DRCompute["Compute Layer<br/>Minimal Footprint<br/>3 nodes standby"]
        DRData["Data Layer<br/>Cross-Region Replication<br/>RTO: 1 hora | RPO: 15 min"]
  end
 subgraph OnPremise["üè¢ Legacy Datacenter<br/>Strangler Fig Migration"]
        Bridge["Direct Connect<br/>1 Gbps Dedicated<br/>Latency: 5-10ms"]
        LegacySystem["Legacy J2EE Monolith<br/>Oracle 12c Database<br/>HSM Crypto Hardware<br/>üî¥ Decomissioning: 12 meses"]
  end
    Users --> CDN
    CDN --> DNS
    DNS -->|"Primary"| RegionPrimary
    DNS -.->|"DR Failover"| RegionDR
    Compute1 <--> Data1
    Compute2 <--> Data2
    Compute3 <--> Data3
    Data1 -.->|"sync replication"| Data2
    Data1 -.->|"async replication"| Data3
    Data2 -.->|"async replication"| Data3
    Compute1 & Compute2 & Compute3 --> SharedSvcs
    RegionPrimary -.->|"Cross-Region<br/>Async Backup"| RegionDR
    Compute1 & Compute2 -.->|"Direct Connect"| Bridge
    Bridge --> LegacySystem
    Monitoring -.->|"monitor all"| Compute1 & Compute2 & Compute3 & Data1

     Users:::user
     CDN:::global
     DNS:::global
     Compute1:::compute
     Compute2:::compute
     Compute3:::compute
     Data1:::data
     Data2:::data
     Data3:::data
     Orchestration:::shared
     Search:::shared
     Monitoring:::shared
     DRCompute:::dr
     DRData:::dr
     Bridge:::legacy
     LegacySystem:::legacy
    classDef user fill:#e8f5e9,stroke:#2e7d32,stroke-width:3px,color:#000
    classDef global fill:#e1f5fe,stroke:#0277bd,stroke-width:3px,color:#000
    classDef compute fill:#fff3e0,stroke:#ef6c00,stroke-width:2px,color:#000
    classDef data fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000
    classDef shared fill:#fff9c4,stroke:#f57f17,stroke-width:2px,color:#000
    classDef dr fill:#ffebee,stroke:#c62828,stroke-width:2px,stroke-dasharray: 5 5,color:#000
    classDef legacy fill:#fce4ec,stroke:#880e4f,stroke-width:2px,color:#000
```

---

## üèóÔ∏è Estrategia de Capacidad por Availability Zone

### Distribuci√≥n de Carga (Active-Active)

**Modelo**: Cada AZ soporta 33% de tr√°fico total (distribuci√≥n uniforme)

| AZ | Compute Nodes | Target TPS | Databases | Status |
|----|---------------|------------|-----------|--------|
| **us-east-1a** | 27 nodes (9c+6m+3g) | 333K TPS | PostgreSQL Primary, Redis Primary, Kafka Leader | ‚úÖ Activo |
| **us-east-1b** | 27 nodes (9c+6m+3g) | 333K TPS | PostgreSQL Standby, Redis Replica, Kafka Follower | ‚úÖ Activo |
| **us-east-1c** | 27 nodes (9c+6m+3g) | 333K TPS | Redis Replica, Kafka Follower, Cassandra/TimescaleDB | ‚úÖ Activo |
| **Total** | **81 nodes** | **1M TPS** | Multi-AZ replication | **99.999% uptime** |

**c** = compute-optimized (c6i.2xlarge), **m** = memory-optimized (r6i.2xlarge), **g** = GPU (g4dn.xlarge)

### Escenarios de Falla

#### Falla de 1 AZ (us-east-1a ca√≠da completa)

```
Estado Normal:
  AZ-A: 333K TPS (33%)
  AZ-B: 333K TPS (33%) 
  AZ-C: 333K TPS (33%)
  Total: 1M TPS

Falla AZ-A:
  AZ-A: 0 TPS (ca√≠do)
  AZ-B: 500K TPS (50%) ‚Üê redistribuido
  AZ-C: 500K TPS (50%) ‚Üê redistribuido
  Total: 1M TPS mantenido
  
Impact:
  - Latencia: +50ms (cross-AZ traffic)
  - PostgreSQL Failover: 2 minutos
  - Redis Promotion: autom√°tica (< 1 min)
  - Kafka Rebalance: autom√°tico (< 30s)
  - Availability: 100% (sin downtime)
```

#### Falla de 2 AZs (catastr√≥fico, probabilidad < 0.001%)

```
Falla AZ-A + AZ-B:
  AZ-C: 1M TPS (100%)
  
Impact:
  - Capacidad: SOBRECARGA (necesita 81 nodes, tiene 27)
  - Degradaci√≥n: Throttling a 400K TPS
  - Recovery: Activar DR region (us-west-2)
```

---

## üåç Estrategia Multi-Regi√≥n (Disaster Recovery)

### Regi√≥n Primaria: us-east-1 (Active)

**Capacidad Total**: 1M TPS  
**Uptime**: 99.999% (5.26 min downtime/a√±o)  
**Latencia**: < 50ms p99 (USA + LATAM)

### Regi√≥n DR: us-west-2 (Passive Standby)

**Capacidad**: 100K TPS (10% de primaria)  
**Estado**: Cold standby (infraestructura m√≠nima)  
**Activaci√≥n**: Manual (decisi√≥n CTO/VP Engineering)

```yaml
DR Configuration:
  Compute:
    EKS Nodes: 3 (minimal footprint)
    Auto-scaling: Disabled (activar en DR)
  
  Data Replication:
    PostgreSQL: Cross-region replica (async, lag < 5s)
    Redis: Backup diario a S3 (restore 15 min)
    Kafka: MirrorMaker 2 (async, lag < 30s)
    Cassandra: Snapshot semanal (restore 1 hora)
    TimescaleDB: Cross-region streaming (lag < 10s)
  
  RPO (Recovery Point Objective): 15 minutos
  RTO (Recovery Time Objective): 1 hora
  
  Costo Optimizaci√≥n:
    - Primaria: $45K/mes (24/7 full capacity)
    - DR: $5K/mes (10% footprint standby)
    - Total: $50K/mes
```

### Activaci√≥n de DR (Procedimiento)

1. **Decisi√≥n**: CTO aprueba failover (regi√≥n primaria completamente inaccesible)
2. **Promoci√≥n de datos** (15 min):
   - PostgreSQL replica ‚Üí Primary
   - Kafka consumers switch topics
   - Redis restore from S3
3. **Escalamiento compute** (30 min):
   - EKS Cluster Autoscaler: 3 ‚Üí 81 nodes
   - Pods deployment: full service replica
4. **DNS Cutover** (5 min):
   - Route53 health check fail
   - Traffic redirect us-west-2
5. **Validaci√≥n** (10 min):
   - Smoke tests
   - Monitor dashboard

**Total RTO**: 1 hora

---

## üìä Dimensionamiento y Escalabilidad

### Compute Layer (EKS)

```yaml
Configuraci√≥n por AZ:
  Compute Nodes (c6i.2xlarge):
    Count: 9 nodes √ó 3 AZ = 27 total
    vCPU: 8 √ó 9 = 72 cores/AZ
    RAM: 16 GB √ó 9 = 144 GB/AZ
    Purpose: Payment, FX, Clearing, Facade, HSM Proxy
  
  Memory Nodes (r6i.2xlarge):
    Count: 6 nodes √ó 3 AZ = 18 total
    vCPU: 8 √ó 6 = 48 cores/AZ
    RAM: 64 GB √ó 6 = 384 GB/AZ
    Purpose: Ledger (Event Sourcing), Temporal.io Workers
  
  GPU Nodes (g4dn.xlarge):
    Count: 3 nodes √ó 3 AZ = 9 total
    GPU: NVIDIA T4 (16GB) √ó 3/AZ
    Purpose: TensorFlow Serving (Fraud ML Model)

Total Cluster:
  Nodes: 81 (27+18+9) √ó 3 AZ
  vCPU: 360 cores total
  RAM: 1.6 TB total
  GPU: 9 √ó T4

Escalamiento Autom√°tico:
  Horizontal Pod Autoscaler (HPA):
    - Min replicas: 3 (1 por AZ)
    - Max replicas: 12 (4 por AZ)
    - Target CPU: 70%
    - Scale up: Inmediato (< 30s)
    - Scale down: Gradual (5 min stabilization)
  
  Cluster Autoscaler:
    - Min nodes: 27/AZ (baseline)
    - Max nodes: 45/AZ (emergencia)
    - Scale up: 2 minutos (AWS API + boot time)
    - Scale down: 10 minutos (drain pods)
```

### Data Layer (Distributed)

```yaml
PostgreSQL (RDS):
  Primary: db.r6g.2xlarge (us-east-1a)
    vCPU: 8, RAM: 64 GB
    Storage: 1 TB gp3 (16K IOPS)
    Connections: 500 concurrent
  
  Standby: db.r6g.2xlarge (us-east-1b)
    Sync Replication: < 100ms lag
    Auto-failover: 2 minutos
  
  Read Replicas: 2 adicionales (CQRS + Analytics)

Kafka (MSK):
  Brokers: 3 (kafka.m5.xlarge, 1/AZ)
  Throughput: 500K msg/s aggregate
  Storage: 500 GB/broker
  Replication: 3x (quorum writes)

Redis (ElastiCache):
  Shards: 3 (cache.r6g.xlarge, 1/AZ)
  Replicas: 6 (2/shard)
  Total Nodes: 9
  Throughput: 200K ops/s aggregate

Cassandra (Self-managed):
  Nodes: 9 (i3.2xlarge, 3/AZ)
  Replication Factor: 3
  Write Throughput: 50K writes/s
  Storage/node: 1.9 TB NVMe SSD

TimescaleDB (Extension PostgreSQL):
  Hypertables: Event Sourcing ledger
  Compression: 10x after 30 days
  Retention: Infinito (compliance)

---

## üîó Integraci√≥n con Legacy (Strangler Fig)

### Direct Connect a Datacenter On-Premise

```yaml
Conectividad:
  Tipo: AWS Direct Connect
  Bandwidth: 1 Gbps dedicado
  Latencia: 5-10ms (Ashburn ‚Üî Legacy DC)
  Redundancia: VPN over Internet (backup, 500 Mbps)
  
Servicios Legacy:
  Oracle Database 12c:
    Size: 2 TB (CORE_SCHEMA)
    Replication: CDC via Debezium ‚Üí Kafka
    Lag target: < 2 segundos
  
  HSM Hardware (Thales nShield):
    Protocol: ISO 8583 over TCP
    Operations: PIN verification, key generation
    Throughput: 2K signatures/s
  
  J2EE Monolith:
    Estado: üî¥ Strangler Fig en progreso
    Traffic: 10% legacy, 90% microservices
    Decommission: 12 meses
```

### Estrategia de Migraci√≥n (Timeline)

| Fase | Duraci√≥n | Traffic Split | Status |
|------|----------|---------------|--------|
| **Fase 1: Coexistencia** | Mes 1-3 | 90% Legacy / 10% Microservices | ‚úÖ Completado |
| **Fase 2: Incremental** | Mes 4-9 | 30% Legacy / 70% Microservices | üîÑ En progreso |
| **Fase 3: Retiro** | Mes 10-12 | 0% Legacy / 100% Microservices | ‚è≥ Planeado |

**Fecha estimada apagado legacy**: Diciembre 2026

---

## üéØ M√©tricas de √âxito (SLIs/SLOs)

### Service Level Objectives

| M√©trica | Sistema Actual (Legacy) | Target Nueva Arquitectura | Status |
|---------|-------------------------|---------------------------|--------|
| **Availability (Uptime)** | 99.5% (43h downtime/a√±o) | 99.999% (5 min downtime/a√±o) | ‚úÖ En camino |
| **Throughput (TPS)** | 2,000 TPS m√°x | 1,000,000 TPS | ‚úÖ 500x mejora |
| **Latency (p99)** | 2,000ms | 200ms | ‚úÖ 10x mejora |
| **Time-to-Deploy** | 6 horas (ventana nocturna) | 15 minutos (rolling update) | ‚úÖ 24x m√°s r√°pido |
| **Recovery Time (RTO)** | 4 horas (backup restore) | 2 minutos (AZ failover) | ‚úÖ 120x m√°s r√°pido |
| **Database Lock Contention** | 40% queries bloqueadas | 0% (database per service) | ‚úÖ Eliminado |

### Monitoreo (Observability Stack)

```yaml
Prometheus + Grafana:
  M√©tricas: 
    - CPU/Memory/Network por pod
    - Request rate, latency, errors (RED method)
    - Database connections, query time
    - Kafka lag, throughput
  
  Dashboards:
    - "Payment Service Overview"
    - "EKS Cluster Health"
    - "Database Performance"
    - "Cross-AZ Latency"
  
  Alerts:
    - Pod OOMKilled (Slack + PagerDuty)
    - Kafka lag > 10s (Slack)
    - Database connections > 80% (PagerDuty)
    - AZ failover event (Slack + Email)

Jaeger (Distributed Tracing):
  Sampling: 1% en prod (100% en dev)
  Retention: 7 d√≠as
  Use Cases:
    - Payment flow end-to-end (7 services)
    - Latency spikes debugging
    - Dependency map visualization

ELK Stack (Logs):
  Logstash: Centralized log aggregation
  Elasticsearch: Log indexing (retention 30 d√≠as)
  Kibana: Log search + visualization
```

---

## üí∞ An√°lisis de Costos (TCO)

### Costo Mensual Estimado

```yaml
Compute (EKS):
  c6i.2xlarge: 27 nodes √ó $0.34/hr √ó 730h = $6,700/mes
  r6i.2xlarge: 18 nodes √ó $0.50/hr √ó 730h = $6,570/mes
  g4dn.xlarge: 9 nodes √ó $0.526/hr √ó 730h = $3,450/mes
  Total Compute: $16,720/mes

Data Layer:
  RDS PostgreSQL: db.r6g.2xlarge √ó 2 = $1,460/mes
  ElastiCache Redis: 9 nodes √ó $0.23/hr = $1,510/mes
  MSK Kafka: 3 brokers √ó $0.21/hr = $460/mes
  EC2 Cassandra: 9 √ó i3.2xlarge √ó $0.624/hr = $4,100/mes
  Total Data: $7,530/mes

Network & Storage:
  NAT Gateways: 3 √ó $45 = $135/mes
  Data Transfer: ~$2,000/mes
  EBS Storage: 10 TB √ó $0.08/GB = $800/mes
  S3 Backups: 50 TB √ó $0.023/GB = $1,150/mes
  Total Network: $4,085/mes

Shared Services:
  CloudFront: $500/mes
  Route53: $50/mes
  CloudWatch/Logs: $800/mes
  Direct Connect: $300/mes (1 Gbps port + data transfer)
  Total Shared: $1,650/mes

DR Region (us-west-2):
  Minimal footprint: $5,000/mes

Total Mensual: $35,000/mes (Primaria) + $5,000 (DR) = $40,000/mes
Total Anual: $480,000/a√±o
```

### Comparaci√≥n vs Sistema Actual

| Concepto | Legacy (On-Premise) | Cloud (AWS) | Diferencia |
|----------|---------------------|-------------|------------|
| **Capex inicial** | $2M (hardware) | $0 (OpEx model) | -$2M |
| **OpEx mensual** | $60K (datacenter, licencias) | $40K (AWS) | -$20K/mes |
| **Escalabilidad** | Manual (6 meses procurement) | Autom√°tica (minutos) | ‚úÖ Agilidad |
| **Disponibilidad** | 99.5% (1 datacenter) | 99.999% (multi-AZ) | +0.499% |
| **Mantenimiento** | 10 SRE full-time | 3 SRE + AWS Managed | -70% headcount |

**ROI**: Payback en 10 meses (ahorro Capex + reducci√≥n OpEx)

---

## üìã Roadmap de Implementaci√≥n

### Q1 2026: Fundaci√≥n

- ‚úÖ Configurar VPC multi-AZ
- ‚úÖ Desplegar EKS cluster (81 nodes)
- ‚úÖ Configurar RDS Multi-AZ + Read Replicas
- ‚úÖ Implementar Kafka MSK + topics
- ‚úÖ Deploy Cassandra cluster (self-managed)
- ‚úÖ Direct Connect a legacy datacenter

### Q2 2026: Migraci√≥n Fase 1 (Coexistencia)

- ‚úÖ Desplegar Payment Service (10% traffic)
- ‚úÖ Legacy Facade (ACL pattern)
- ‚úÖ CDC Adapter (Oracle ‚Üí Kafka)
- üîÑ Monitoreo dual (legacy + microservices)
- üîÑ Chaos Engineering tests

### Q3 2026: Migraci√≥n Fase 2 (Incremental)

- üîÑ Aumentar traffic 10% ‚Üí 70%
- üîÑ Desplegar Fraud, FX, Clearing services
- üîÑ Temporal.io Saga orchestration
- ‚è≥ Reducir dependencia Oracle

### Q4 2026: Migraci√≥n Fase 3 (Retiro)

- ‚è≥ Traffic 70% ‚Üí 100% microservices
- ‚è≥ Apagar legacy J2EE monolith
- ‚è≥ Archivar Oracle a S3 Glacier
- ‚è≥ Decommission on-premise datacenter

### Q1 2027: Optimizaci√≥n

- ‚è≥ Activar regi√≥n DR (us-west-2)
- ‚è≥ Implementar multi-cloud (Azure backup)
- ‚è≥ Cost optimization (Spot instances, Reserved capacity)

---

**Referencias**:
- Detalle de deployment: [Despliegue.md](../03-Dise√±o-Tecnico/3.2-UML/Despliegue.md)
- Seguridad de red: [Infraestructura.md](../03-Dise√±o-Tecnico/3.2-UML/Infraestructura.md)
- Resiliencia: [4.2-Resiliencia.md](4.2-Resiliencia.md)

---

**Fecha de Propuesta**: 24 de diciembre de 2025  
**√öltima Actualizaci√≥n**: 24 de diciembre de 2025
